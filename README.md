# API Pipeline - E-commerce Data

Pipeline **ETL (Extract, Transform, Load)** desarrollado en Python que consume datos de una API de e-commerce, los transforma y los guarda en formato **Parquet**, tanto de forma consolidada como particionada por fecha.

Este proyecto fue realizado como parte del **Bootcamp de Data Engineering â€“ Ian Saura** y sigue buenas prÃ¡cticas profesionales: uso de variables de entorno, manejo de errores, logging y estructura modular.

---

## ğŸ“Œ DescripciÃ³n del Pipeline

El pipeline realiza los siguientes pasos:

1. **Extract**: Consume datos desde una API REST protegida por token.
2. **Transform**: Limpia y transforma los datos usando pandas, agregando columnas derivadas y validaciones.
3. **Load**: Guarda los datos en formato Parquet:

   * Un archivo consolidado (todo junto)
   * Datos particionados por aÃ±o y mes para optimizar consultas

---

## âš™ï¸ Setup

### 1ï¸âƒ£ Clonar el repositorio

```bash
git clone <url-del-repositorio>
cd api-pipeline
```

### 2ï¸âƒ£ Crear el archivo `.env`

Crear un archivo `.env` en la raÃ­z del proyecto con el siguiente contenido:

```
API_TOKEN=tu_token_aqui
API_BASE_URL=https://iansaura.com/api
```

âš ï¸ **Importante**: El archivo `.env` no debe subirse a GitHub. EstÃ¡ incluido en `.gitignore`.

---

### 3ï¸âƒ£ Instalar dependencias

```bash
pip install -r requirements.txt
```

---

## â–¶ï¸ Uso

Para ejecutar el pipeline completo:

```bash
python main.py
```

El pipeline incluye manejo de errores y logging para facilitar el monitoreo y debugging.

---

## ğŸ“‚ Output

Los datos generados se guardan en la carpeta `output/`:

```
output/
â”œâ”€â”€ orders/
â”‚   â””â”€â”€ order_year=2024/
â”‚       â”œâ”€â”€ order_month=2024-01/
â”‚       â”‚   â””â”€â”€ data.parquet
â”‚       â”œâ”€â”€ order_month=2024-02/
â”‚       â”‚   â””â”€â”€ data.parquet
â”‚       â””â”€â”€ ...
â””â”€â”€ orders_all.parquet
```

### ğŸ“ Tipos de salida

* **orders_all.parquet**: archivo consolidado con todas las Ã³rdenes (Ãºtil para anÃ¡lisis exploratorio y debugging).
* **orders/**: datos particionados por aÃ±o y mes (pensado para consultas eficientes en producciÃ³n).

---

## ğŸ³ Docker

Este proyecto puede ejecutarse dentro de un container Docker para garantizar un entorno reproducible.

### Build de la imagen
Desde la raÃ­z del proyecto:

```bash
docker build -t api-pipeline .
```

---

## ğŸ› ï¸ TecnologÃ­as utilizadas

* Python 3
* requests
* pandas
* pyarrow
* python-dotenv

---

## ğŸ‘©â€ğŸ’» Autora

MarÃ­a Victoria D'Ercole
